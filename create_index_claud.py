from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes import SearchIndexerClient, SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SearchFieldDataType, SimpleField, SearchableField,
    SearchIndexerDataSourceConnection, SearchIndexerDataContainer,
    SearchIndexerSkillset, SearchIndexer,
    InputFieldMappingEntry, OutputFieldMappingEntry,
    FieldMapping, OcrSkill, LanguageDetectionSkill, KeyPhraseExtractionSkill,
    MergeSkill, SplitSkill
)
from azure.search.documents import SearchClient
import os
from dotenv import load_dotenv
import time

load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í´ë”ëª…ê³¼ ì¸ë±ìŠ¤ëª… ê°€ì ¸ì˜¤ê¸°
folder_name = os.getenv("FOLDER_NAME")
index_name = os.getenv("INDEX_NAME")

if not folder_name:
    print("âŒ FOLDER_NAME í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    exit(1)

if not index_name:
    # ê¸°ë³¸ ì¸ë±ìŠ¤ëª… ìƒì„± (í´ë”ëª…-index)
    index_name = f"{folder_name}-index"

print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {folder_name}")
print(f"ğŸ“Š ìƒì„±í•  ì¸ë±ìŠ¤: {index_name}")

# í™˜ê²½ ë³€ìˆ˜
endpoint = f"https://{os.getenv('AZURE_SEARCH_SERVICE_NAME')}.search.windows.net"
credential = AzureKeyCredential(os.getenv("AZURE_SEARCH_SERVICE_ADMIN_KEY"))

container_name = os.getenv("AZURE_STORAGE_CONTAINER_NAME")
data_source_name = f"{folder_name}-datasource"
skillset_name = f"{folder_name}-skillset"
indexer_name = "azureblob-indexer"  # ê¸°ì¡´ indexer ì‚¬ìš©

# í´ë¼ì´ì–¸íŠ¸
index_client = SearchIndexClient(endpoint, credential)
indexer_client = SearchIndexerClient(endpoint, credential)

print("ğŸ” ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸...")

# ê¸°ì¡´ indexer ì¡´ì¬ ì—¬ë¶€ í™•ì¸
try:
    existing_indexer = indexer_client.get_indexer(indexer_name)
    print(f"âœ… ê¸°ì¡´ Indexer '{indexer_name}' ë°œê²¬")
except Exception as e:
    print(f"âŒ í•„ìˆ˜ Indexer '{indexer_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print(f"âŒ ì˜¤ë¥˜ ì„¸ë¶€ì •ë³´: {e}")
    print(f"ğŸ’¡ ë¨¼ì € '{indexer_name}' ì¸ë±ì„œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")
    exit(1)

# ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (indexerëŠ” ì œì™¸)
print("ğŸ§¹ ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
for resource_type, resource_name, delete_func in [
    ("skillset", skillset_name, indexer_client.delete_skillset),
    ("data source", data_source_name, indexer_client.delete_data_source_connection),
    ("index", index_name, index_client.delete_index)
]:
    try:
        delete_func(resource_name)
        print(f"âœ… ê¸°ì¡´ {resource_type} '{resource_name}' ì‚­ì œë¨")
    except:
        print(f"â„¹ï¸ {resource_type} '{resource_name}' ì—†ìŒ (ì •ìƒ)")

time.sleep(2)  # ì‚­ì œ ëŒ€ê¸°

print("\nğŸ—ï¸ ìƒˆë¡œìš´ ë¦¬ì†ŒìŠ¤ ìƒì„± ì¤‘...")

# 1. í–¥ìƒëœ ì¸ë±ìŠ¤ ìƒì„±
index = SearchIndex(
    name=index_name,
    fields=[
        SimpleField(name="id", type=SearchFieldDataType.String, key=True),
        SearchableField(name="content", type=SearchFieldDataType.String, 
                       searchable=True, retrievable=True, analyzer_name="ko.microsoft"),
        SearchableField(name="merged_content", type=SearchFieldDataType.String,
                       searchable=True, retrievable=True, analyzer_name="ko.microsoft"),
        SearchableField(name="text", type=SearchFieldDataType.Collection(SearchFieldDataType.String),
                       searchable=True, retrievable=True),
        SimpleField(name="languageCode", type=SearchFieldDataType.String, 
                   filterable=True, retrievable=True),
        SearchableField(
            name="keyPhrases",
            type=SearchFieldDataType.Collection(SearchFieldDataType.String),
            searchable=True, retrievable=True
        ),
        SimpleField(name="metadata_storage_name", type=SearchFieldDataType.String, 
                   filterable=True, retrievable=True, sortable=True),
        SimpleField(name="metadata_storage_path", type=SearchFieldDataType.String,
                   retrievable=True),
        SimpleField(name="metadata_storage_size", type=SearchFieldDataType.Int64,
                   filterable=True, retrievable=True, sortable=True),
        SimpleField(name="metadata_storage_last_modified", type=SearchFieldDataType.DateTimeOffset,
                   filterable=True, retrievable=True, sortable=True),
    ]
)

try:
    index_client.create_index(index)
    print(f"âœ… Index '{index_name}' ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"âŒ Index ìƒì„± ì‹¤íŒ¨: {e}")
    exit(1)

# 2. ë°ì´í„° ì†ŒìŠ¤ ìƒì„± (íŠ¹ì • í´ë”ë§Œ ëŒ€ìƒ)
data_source = SearchIndexerDataSourceConnection(
    name=data_source_name,
    type="azureblob",
    connection_string=os.getenv("AZURE_STORAGE_CONNECTION_STRING"),
    container=SearchIndexerDataContainer(
        name=container_name,
        query=folder_name  # íŠ¹ì • í´ë”ë§Œ ì¸ë±ì‹±
    ),
    description=f"PDF files in folder '{folder_name}' for indexing"
)

try:
    indexer_client.create_data_source_connection(data_source)
    print(f"âœ… Data source '{data_source_name}' ìƒì„± ì™„ë£Œ (í´ë”: {folder_name})")
except Exception as e:
    print(f"âŒ Data source ìƒì„± ì‹¤íŒ¨: {e}")
    exit(1)

# 3. í–¥ìƒëœ Skillset ìƒì„±
skillset = SearchIndexerSkillset(
    name=skillset_name,
    description=f"Enhanced PDF OCR and analysis for folder '{folder_name}'",
    skills=[
        # OCR Skill - ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        OcrSkill(
            name="ocr-skill",
            description="Extract text from images in PDF",
            context="/document/normalized_images/*",
            inputs=[
                InputFieldMappingEntry(name="image", source="/document/normalized_images/*")
            ],
            outputs=[
                OutputFieldMappingEntry(name="text", target_name="text")
            ]
        ),
        
        # Merge Skill - ì›ë³¸ í…ìŠ¤íŠ¸ì™€ OCR í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        MergeSkill(
            name="merge-skill",
            description="Merge original content with OCR text",
            context="/document",
            inputs=[
                InputFieldMappingEntry(name="text", source="/document/content"),
                InputFieldMappingEntry(name="itemsToInsert", source="/document/normalized_images/*/text")
            ],
            outputs=[
                OutputFieldMappingEntry(name="mergedText", target_name="merged_content")
            ]
        ),
        
        # Language Detection - í•©ì³ì§„ í…ìŠ¤íŠ¸ ê¸°ì¤€
        LanguageDetectionSkill(
            name="language-skill",
            description="Detect language from merged text",
            context="/document",
            inputs=[
                InputFieldMappingEntry(name="text", source="/document/merged_content")
            ],
            outputs=[
                OutputFieldMappingEntry(name="languageCode", target_name="languageCode")
            ]
        ),
        
        # Key Phrase Extraction - í•©ì³ì§„ í…ìŠ¤íŠ¸ ê¸°ì¤€
        KeyPhraseExtractionSkill(
            name="keyphrase-skill",
            description="Extract key phrases from merged text",
            context="/document",
            inputs=[
                InputFieldMappingEntry(name="text", source="/document/merged_content"),
                InputFieldMappingEntry(name="languageCode", source="/document/languageCode")
            ],
            outputs=[
                OutputFieldMappingEntry(name="keyPhrases", target_name="keyPhrases")
            ]
        )
    ]
)

try:
    indexer_client.create_skillset(skillset)
    print(f"âœ… Skillset '{skillset_name}' ìƒì„± ì™„ë£Œ")
except Exception as e:
    print(f"âŒ Skillset ìƒì„± ì‹¤íŒ¨: {e}")
    exit(1)

# 4. ê¸°ì¡´ Indexer ì„¤ì • í™•ì¸
print(f"ğŸ” ê¸°ì¡´ Indexer '{indexer_name}' ì„¤ì • í™•ì¸...")

try:
    print(f"ğŸ“‹ í˜„ì¬ Indexer ì„¤ì •:")
    print(f"  - Data Source: {existing_indexer.data_source_name}")
    print(f"  - Target Index: {existing_indexer.target_index_name}")
    print(f"  - Skillset: {existing_indexer.skillset_name}")
    print(f"  - Field Mappings: {len(existing_indexer.field_mappings or [])}ê°œ")
    print(f"  - Output Mappings: {len(existing_indexer.output_field_mappings or [])}ê°œ")
    
    print(f"ğŸ’¡ ê¸°ì¡´ Indexer '{indexer_name}'ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print(f"âš ï¸ ì£¼ì˜: í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ì¸ë±ì‹±ì´ ì§„í–‰ë©ë‹ˆë‹¤.")
    
except Exception as e:
    print(f"âŒ Indexer ì„¤ì • í™•ì¸ ì‹¤íŒ¨: {e}")
    exit(1)

# 5. Indexer ë¦¬ì…‹ ë° ì‹¤í–‰ (ê¸°ì¡´ ì„¤ì •ìœ¼ë¡œ)
print(f"\nğŸ”„ Indexer '{indexer_name}' ë¦¬ì…‹ ë° ì‹¤í–‰...")
print(f"âš ï¸ ì£¼ì˜: ê¸°ì¡´ ì¸ë±ì„œ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
print(f"ğŸ“Š ìƒˆë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ '{index_name}'ì™€ í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")

try:
    # indexer ë¦¬ì…‹ (ê¸°ì¡´ ì¸ë±ì‹± ìƒíƒœ ì´ˆê¸°í™”)
    indexer_client.reset_indexer(indexer_name)
    print(f"ğŸ”„ Indexer '{indexer_name}' ë¦¬ì…‹ ì™„ë£Œ")
    
    time.sleep(3)  # ë¦¬ì…‹ ëŒ€ê¸°
    
    # indexer ì‹¤í–‰
    indexer_client.run_indexer(indexer_name)
    print(f"ğŸš€ Indexer '{indexer_name}' ì‹¤í–‰ ì‹œì‘")
except Exception as e:
    print(f"âŒ Indexer ë¦¬ì…‹/ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    print(f"ğŸ’¡ íŒíŠ¸: ê¸°ì¡´ ì¸ë±ì„œ ì„¤ì •ì´ ìƒˆ ì¸ë±ìŠ¤ì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# 6. í–¥ìƒëœ ìƒíƒœ ëª¨ë‹ˆí„°ë§
print("â³ Indexer ì‹¤í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§...")
max_wait_cycles = 30  # ìµœëŒ€ 10ë¶„ ëŒ€ê¸° (20ì´ˆ * 30)

for i in range(max_wait_cycles):
    time.sleep(20)  # 20ì´ˆë§ˆë‹¤ ì²´í¬
    
    try:
        status = indexer_client.get_indexer_status(indexer_name)
        current_status = status.status
        print(f"ğŸ“Š ìƒíƒœ ì²´í¬ {i+1}/{max_wait_cycles}: {current_status}")
        
        if status.last_result:
            print(f"  ğŸ“… ì‹œì‘: {status.last_result.start_time}")
            print(f"  ğŸ“… ì¢…ë£Œ: {status.last_result.end_time}")
            print(f"  ğŸ“„ ì²˜ë¦¬ëœ ë¬¸ì„œ: {status.last_result.item_count}")
            print(f"  âŒ ì‹¤íŒ¨í•œ ë¬¸ì„œ: {status.last_result.failed_item_count}")
            
            if status.last_result.end_time:
                if status.last_result.status == "success":
                    print("âœ… Indexer ì‹¤í–‰ ì„±ê³µ!")
                    break
                else:
                    print(f"âŒ Indexer ì‹¤íŒ¨: {status.last_result.error_message}")
                    if status.last_result.errors:
                        for err in status.last_result.errors:
                            print(f"âš ï¸ ì„¸ë¶€ ì—ëŸ¬: {err.name} - {err.error_message}")
                    break
        print("  " + "-" * 50)
    except Exception as e:
        print(f"âš ï¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")

# 7. ê²°ê³¼ í™•ì¸ ë° ê²€ì¦
print(f"\nğŸ” Index '{index_name}' ë‚´ìš© í™•ì¸:")
search_client = SearchClient(
    endpoint=endpoint,
    index_name=index_name,
    credential=credential
)

try:
    # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
    results = search_client.search(search_text="*", top=5, include_total_count=True)
    total_count = results.get_count()
    print(f"ğŸ“Š ì´ ì¸ë±ì‹±ëœ ë¬¸ì„œ ìˆ˜: {total_count}")
    
    if total_count > 0:
        print(f"\nğŸ“‹ ìƒìœ„ {min(5, total_count)}ê°œ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°:")
        
        for idx, doc in enumerate(results, 1):
            print(f"\n[ë¬¸ì„œ {idx}]")
            print(f"ğŸ“„ ID: {doc['id']}")
            print(f"ğŸ“ íŒŒì¼ëª…: {doc.get('metadata_storage_name', 'ì—†ìŒ')}")
            print(f"ğŸ“ íŒŒì¼í¬ê¸°: {doc.get('metadata_storage_size', 0):,} bytes")
            print(f"ğŸ“… ìˆ˜ì •ì¼: {doc.get('metadata_storage_last_modified', 'ì—†ìŒ')}")
            
            # ì›ë³¸ ì½˜í…ì¸ 
            content = doc.get("content", "")
            if content:
                print(f"ğŸ“ ì›ë³¸ Content: {content[:150]}{'...' if len(content) > 150 else ''}")
            
            # í•©ì³ì§„ ì½˜í…ì¸  (OCR + ì›ë³¸)
            merged_content = doc.get("merged_content", "")
            if merged_content:
                print(f"ğŸ”— í•©ì³ì§„ Content: {merged_content[:150]}{'...' if len(merged_content) > 150 else ''}")
            
            # OCR í…ìŠ¤íŠ¸ ë°°ì—´
            ocr_texts = doc.get("text", [])
            if ocr_texts:
                print(f"ğŸ” OCR Text ê°œìˆ˜: {len(ocr_texts)}")
                if ocr_texts:
                    first_ocr = ocr_texts[0] if isinstance(ocr_texts[0], str) else str(ocr_texts[0])
                    print(f"ğŸ” ì²« ë²ˆì§¸ OCR: {first_ocr[:100]}{'...' if len(first_ocr) > 100 else ''}")
            
            print(f"ğŸŒ Language: {doc.get('languageCode', 'ì—†ìŒ')}")
            key_phrases = doc.get('keyPhrases', [])
            if key_phrases:
                print(f"ğŸ”‘ Key Phrases ({len(key_phrases)}ê°œ): {', '.join(key_phrases[:5])}{'...' if len(key_phrases) > 5 else ''}")
            
            print("=" * 80)
    else:
        print("âš ï¸ ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("  - í´ë” ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€")
        print("  - í•´ë‹¹ í´ë”ì— PDF íŒŒì¼ì´ ìˆëŠ”ì§€")
        print("  - Azure Storage ì—°ê²°ì´ ì •ìƒì¸ì§€")
        
except Exception as e:
    print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

print(f"\nğŸ‰ ì„¤ì • ì™„ë£Œ!")
print(f"ğŸ“‚ ëŒ€ìƒ í´ë”: {folder_name}")
print(f"ğŸ“Š ìƒì„±ëœ ì¸ë±ìŠ¤: {index_name}")
print(f"ğŸ”§ ì‚¬ìš©ëœ ì¸ë±ì„œ: {indexer_name}")
print(f"ğŸ’¡ ì´ì œ ê²€ìƒ‰ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ index_name '{index_name}'ì„ ì‚¬ìš©í•˜ì„¸ìš”.")

# 8. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš°)
if total_count > 0:
    print(f"\nğŸ” ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    try:
        test_results = search_client.search(search_text="*", top=1)
        for doc in test_results:
            content = doc.get('merged_content', '') or doc.get('content', '')
            if content:
                # ì²« ë²ˆì§¸ ë‹¨ì–´ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                first_words = ' '.join(content.split()[:2])
                if first_words.strip():
                    print(f"ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ì–´: '{first_words}'")
                    search_results = search_client.search(search_text=first_words, top=3)
                    print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(list(search_results))}ê°œ ë¬¸ì„œ ë°œê²¬")
                    break
    except Exception as e:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

print("\nâœ¨ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì™„ë£Œ!")